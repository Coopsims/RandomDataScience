{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Data importing\n",
    "dataFrame = pd.read_csv('./dataset.csv')\n",
    "\n",
    "# variable I will try to predict: popularity\n",
    "# the dataset is taken from: https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/\n",
    "\n",
    "# Label encoding 'track_genre'\n",
    "le = LabelEncoder()\n",
    "dataFrame['track_genre_encoded'] = le.fit_transform(dataFrame['track_genre'])\n",
    "\n",
    "# Mapping of encoded labels\n",
    "label_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Label to integer mapping:\", label_mapping)\n",
    "\n",
    "# Drop the original 'track_genre' column\n",
    "dataFrame = dataFrame.drop('track_genre', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Label to integer mapping: {'acoustic': 0, 'afrobeat': 1, 'alt-rock': 2, 'alternative': 3, 'ambient': 4, 'anime': 5, 'black-metal': 6, 'bluegrass': 7, 'blues': 8, 'brazil': 9, 'breakbeat': 10, 'british': 11, 'cantopop': 12, 'chicago-house': 13, 'children': 14, 'chill': 15, 'classical': 16, 'club': 17, 'comedy': 18, 'country': 19, 'dance': 20, 'dancehall': 21, 'death-metal': 22, 'deep-house': 23, 'detroit-techno': 24, 'disco': 25, 'disney': 26, 'drum-and-bass': 27, 'dub': 28, 'dubstep': 29, 'edm': 30, 'electro': 31, 'electronic': 32, 'emo': 33, 'folk': 34, 'forro': 35, 'french': 36, 'funk': 37, 'garage': 38, 'german': 39, 'gospel': 40, 'goth': 41, 'grindcore': 42, 'groove': 43, 'grunge': 44, 'guitar': 45, 'happy': 46, 'hard-rock': 47, 'hardcore': 48, 'hardstyle': 49, 'heavy-metal': 50, 'hip-hop': 51, 'honky-tonk': 52, 'house': 53, 'idm': 54, 'indian': 55, 'indie': 56, 'indie-pop': 57, 'industrial': 58, 'iranian': 59, 'j-dance': 60, 'j-idol': 61, 'j-pop': 62, 'j-rock': 63, 'jazz': 64, 'k-pop': 65, 'kids': 66, 'latin': 67, 'latino': 68, 'malay': 69, 'mandopop': 70, 'metal': 71, 'metalcore': 72, 'minimal-techno': 73, 'mpb': 74, 'new-age': 75, 'opera': 76, 'pagode': 77, 'party': 78, 'piano': 79, 'pop': 80, 'pop-film': 81, 'power-pop': 82, 'progressive-house': 83, 'psych-rock': 84, 'punk': 85, 'punk-rock': 86, 'r-n-b': 87, 'reggae': 88, 'reggaeton': 89, 'rock': 90, 'rock-n-roll': 91, 'rockabilly': 92, 'romance': 93, 'sad': 94, 'salsa': 95, 'samba': 96, 'sertanejo': 97, 'show-tunes': 98, 'singer-songwriter': 99, 'ska': 100, 'sleep': 101, 'songwriter': 102, 'soul': 103, 'spanish': 104, 'study': 105, 'swedish': 106, 'synth-pop': 107, 'tango': 108, 'techno': 109, 'trance': 110, 'trip-hop': 111, 'turkish': 112, 'world-music': 113}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = ['track_genre_encoded', 'danceability', 'energy', 'key', 'loudness','mode', 'speechiness','acousticness','instrumentalness', 'liveness','valence','tempo', 'time_signature', 'explicit']\n",
    "target = ['popularity']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(dataFrame[inputs], dataFrame[target], test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for k in range(1,16):\n",
    "    model = neighbors.KNeighborsRegressor(k)\n",
    "    model.fit(xtrain,ytrain)\n",
    "    print(f'Model Score for {k} classifiers: ', model.score(xtest, ytest))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using KNC the best R-Squared I can get is a 0.15 so now I will move onto a neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "#Data importing\n",
    "# Data importing\n",
    "dataFrame = pd.read_csv('./dataset.csv')\n",
    "\n",
    "# Label encoding 'track_genre'\n",
    "le = LabelEncoder()\n",
    "dataFrame['track_genre_encoded'] = le.fit_transform(dataFrame['track_genre'])\n",
    "\n",
    "# Drop the columns\n",
    "dataFrame = dataFrame.drop(columns=['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'track_genre'])\n",
    "\n",
    "# Split into features and target\n",
    "X = dataFrame.drop(columns=['popularity'])\n",
    "y = dataFrame['popularity']\n",
    "\n",
    "# Convert to tensor\n",
    "X_tensor = tf.convert_to_tensor(X.values, dtype=tf.float32)\n",
    "y_tensor = tf.convert_to_tensor(y.values, dtype=tf.float32)\n",
    "\n",
    "# Calculate the index at which to split the dataset\n",
    "train_size = int(0.8 * len(X))\n",
    "\n",
    "# Manually slice the tensors for train/test sets\n",
    "x_train_tensor = X_tensor[:train_size]\n",
    "y_train_tensor = y_tensor[:train_size]\n",
    "x_test_tensor = X_tensor[train_size:]\n",
    "y_test_tensor = y_tensor[train_size:]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(mse_loss(y_pred, y_true))\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "def r_squared(y_true, y_pred):\n",
    "    residual = tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred)))\n",
    "    total = tf.reduce_sum(tf.square(tf.subtract(y_true, tf.reduce_mean(y_true))))\n",
    "    r2 = tf.subtract(1.0, tf.divide(residual, total))\n",
    "    return r2\n",
    "\n",
    "# Used to trace history of RMSE as the neural network gets trained\n",
    "class RMSEHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.modelRMSE = []\n",
    "        self.validationRMSE = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        self.modelRMSE.append(logs.get('rmse'))\n",
    "        self.validationRMSE.append(logs.get('val_rmse'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1460/1460 [==============================] - 2s 1ms/step - loss: 3238531.5000 - r_squared: -5857.9282 - rmse: 263.3191 - val_loss: 968.2595 - val_r_squared: -54.1650 - val_rmse: 28.1987\n",
      "Epoch 2/500\n",
      "1460/1460 [==============================] - 1s 933us/step - loss: 1279.4233 - r_squared: -1.5719 - rmse: 32.1951 - val_loss: 1808.6515 - val_r_squared: -265.1624 - val_rmse: 40.1310\n",
      "Epoch 3/500\n",
      "1460/1460 [==============================] - 1s 899us/step - loss: 4132.0972 - r_squared: -7.4352 - rmse: 44.9316 - val_loss: 882366.1250 - val_r_squared: -115700.5078 - val_rmse: 908.4353\n",
      "Epoch 4/500\n",
      "1460/1460 [==============================] - 1s 902us/step - loss: 97545.0000 - r_squared: -196.3405 - rmse: 125.3006 - val_loss: 782.9655 - val_r_squared: -92.6610 - val_rmse: 26.3345\n",
      "Epoch 5/500\n",
      "1460/1460 [==============================] - 1s 894us/step - loss: 113872.4141 - r_squared: -226.2620 - rmse: 100.4903 - val_loss: 1684.5358 - val_r_squared: -128.1632 - val_rmse: 38.0736\n",
      "Epoch 6/500\n",
      "1460/1460 [==============================] - 1s 891us/step - loss: 13232.8955 - r_squared: -26.4314 - rmse: 62.8519 - val_loss: 2141.0876 - val_r_squared: -311.8477 - val_rmse: 43.6642\n",
      "Epoch 7/500\n",
      "1460/1460 [==============================] - 1s 942us/step - loss: 71246.6406 - r_squared: -140.7142 - rmse: 96.3023 - val_loss: 2628.8535 - val_r_squared: -233.9523 - val_rmse: 48.0910\n",
      "Epoch 8/500\n",
      "1460/1460 [==============================] - 1s 909us/step - loss: 36677.7070 - r_squared: -75.1634 - rmse: 66.7533 - val_loss: 680.6300 - val_r_squared: -31.2657 - val_rmse: 23.5565\n",
      "Epoch 9/500\n",
      "1460/1460 [==============================] - 1s 898us/step - loss: 11246.4229 - r_squared: -21.9243 - rmse: 52.7336 - val_loss: 1679.4642 - val_r_squared: -238.3837 - val_rmse: 38.7634\n",
      "Epoch 10/500\n",
      "1460/1460 [==============================] - 1s 892us/step - loss: 25942.2363 - r_squared: -58.8616 - rmse: 58.1227 - val_loss: 226240.1406 - val_r_squared: -30791.0059 - val_rmse: 459.5909\n",
      "Epoch 11/500\n",
      "1460/1460 [==============================] - 1s 887us/step - loss: 33846.8867 - r_squared: -68.1625 - rmse: 77.9494 - val_loss: 611.9421 - val_r_squared: -27.9557 - val_rmse: 22.4471\n",
      "Epoch 12/500\n",
      "1460/1460 [==============================] - 1s 931us/step - loss: 50188.0508 - r_squared: -95.6898 - rmse: 58.8771 - val_loss: 578.9836 - val_r_squared: -27.3572 - val_rmse: 21.8904\n",
      "Epoch 13/500\n",
      "1460/1460 [==============================] - 1s 920us/step - loss: 1646.9824 - r_squared: -2.2952 - rmse: 32.7886 - val_loss: 589.1964 - val_r_squared: -27.4716 - val_rmse: 22.0213\n",
      "Epoch 14/500\n",
      "1460/1460 [==============================] - 1s 903us/step - loss: 12139.9375 - r_squared: -23.6032 - rmse: 49.6764 - val_loss: 539.2450 - val_r_squared: -31.8994 - val_rmse: 21.2792\n",
      "Epoch 15/500\n",
      "1460/1460 [==============================] - 1s 918us/step - loss: 18672.9570 - r_squared: -36.8235 - rmse: 48.5685 - val_loss: 539.6724 - val_r_squared: -31.6649 - val_rmse: 21.3368\n",
      "Epoch 16/500\n",
      "1460/1460 [==============================] - 1s 895us/step - loss: 1349.8242 - r_squared: -1.6995 - rmse: 31.1082 - val_loss: 2345.2981 - val_r_squared: -204.4510 - val_rmse: 45.3424\n",
      "Epoch 17/500\n",
      "1460/1460 [==============================] - 1s 930us/step - loss: 6568.0103 - r_squared: -12.0705 - rmse: 41.7591 - val_loss: 817.8300 - val_r_squared: -95.4751 - val_rmse: 26.8887\n",
      "Epoch 18/500\n",
      "1460/1460 [==============================] - 1s 908us/step - loss: 7085.5454 - r_squared: -13.3442 - rmse: 40.8061 - val_loss: 548.8859 - val_r_squared: -39.1502 - val_rmse: 21.5664\n",
      "Epoch 19/500\n",
      "1460/1460 [==============================] - 1s 905us/step - loss: 1670.6768 - r_squared: -2.3844 - rmse: 32.5636 - val_loss: 1569.6143 - val_r_squared: -118.4938 - val_rmse: 36.7089\n",
      "Epoch 20/500\n",
      "1460/1460 [==============================] - 1s 898us/step - loss: 6093.1567 - r_squared: -11.2621 - rmse: 44.0690 - val_loss: 1006.1067 - val_r_squared: -61.4917 - val_rmse: 29.0128\n",
      "Epoch 21/500\n",
      "1460/1460 [==============================] - 1s 908us/step - loss: 6333.5151 - r_squared: -11.6491 - rmse: 46.7660 - val_loss: 716.6136 - val_r_squared: -76.5293 - val_rmse: 24.9632\n",
      "Epoch 22/500\n",
      "1460/1460 [==============================] - 1s 932us/step - loss: 3153.4238 - r_squared: -5.4827 - rmse: 37.2949 - val_loss: 669.4285 - val_r_squared: -67.5445 - val_rmse: 24.0527\n",
      "Epoch 23/500\n",
      "1460/1460 [==============================] - 1s 998us/step - loss: 1052.7758 - r_squared: -1.1063 - rmse: 29.1070 - val_loss: 566.8906 - val_r_squared: -44.6934 - val_rmse: 22.0021\n",
      "Epoch 24/500\n",
      "1460/1460 [==============================] - 1s 908us/step - loss: 3077.4446 - r_squared: -5.1821 - rmse: 33.9008 - val_loss: 825.1813 - val_r_squared: -97.2414 - val_rmse: 26.8960\n",
      "Epoch 25/500\n",
      "1460/1460 [==============================] - 1s 919us/step - loss: 1245.2864 - r_squared: -1.4971 - rmse: 29.8941 - val_loss: 574.3999 - val_r_squared: -45.9925 - val_rmse: 22.1975\n",
      "Epoch 26/500\n",
      "1460/1460 [==============================] - 1s 918us/step - loss: 6608.8623 - r_squared: -12.4322 - rmse: 36.8161 - val_loss: 607.4381 - val_r_squared: -26.6244 - val_rmse: 22.4422\n",
      "Epoch 27/500\n",
      "1460/1460 [==============================] - 1s 934us/step - loss: 654.4507 - r_squared: -0.3049 - rmse: 25.0954 - val_loss: 592.1965 - val_r_squared: -50.4397 - val_rmse: 22.5580\n",
      "Epoch 28/500\n",
      "1460/1460 [==============================] - 1s 921us/step - loss: 903.2455 - r_squared: -0.8206 - rmse: 27.0356 - val_loss: 568.4383 - val_r_squared: -44.9634 - val_rmse: 22.0707\n",
      "Epoch 29/500\n",
      "1460/1460 [==============================] - 1s 903us/step - loss: 722.9674 - r_squared: -0.4464 - rmse: 25.4473 - val_loss: 525.3520 - val_r_squared: -31.6437 - val_rmse: 21.0845\n",
      "Epoch 30/500\n",
      "1460/1460 [==============================] - 1s 932us/step - loss: 629.8926 - r_squared: -0.2616 - rmse: 24.4357 - val_loss: 1421.9844 - val_r_squared: -196.1660 - val_rmse: 35.5973\n",
      "Epoch 31/500\n",
      "1460/1460 [==============================] - 1s 918us/step - loss: 627.6439 - r_squared: -0.2534 - rmse: 24.4895 - val_loss: 517.2805 - val_r_squared: -28.8655 - val_rmse: 20.8106\n",
      "Epoch 32/500\n",
      "1460/1460 [==============================] - 1s 983us/step - loss: 558.8541 - r_squared: -0.1154 - rmse: 23.5131 - val_loss: 514.3375 - val_r_squared: -28.1108 - val_rmse: 20.6867\n",
      "Epoch 33/500\n",
      "1460/1460 [==============================] - 1s 948us/step - loss: 636.1829 - r_squared: -0.2805 - rmse: 24.5448 - val_loss: 2630.9946 - val_r_squared: -239.5693 - val_rmse: 48.2416\n",
      "Epoch 34/500\n",
      "1460/1460 [==============================] - 1s 922us/step - loss: 641.9296 - r_squared: -0.2769 - rmse: 24.3413 - val_loss: 548.6131 - val_r_squared: -41.5951 - val_rmse: 21.5605\n",
      "Epoch 35/500\n",
      "1460/1460 [==============================] - 1s 931us/step - loss: 545.0139 - r_squared: -0.0871 - rmse: 23.2309 - val_loss: 546.5046 - val_r_squared: -41.0068 - val_rmse: 21.5091\n",
      "Epoch 36/500\n",
      "1460/1460 [==============================] - 1s 943us/step - loss: 527.0554 - r_squared: -0.0516 - rmse: 22.8872 - val_loss: 512.6766 - val_r_squared: -27.3996 - val_rmse: 20.5944\n",
      "Epoch 37/500\n",
      "1460/1460 [==============================] - 1s 956us/step - loss: 518.1085 - r_squared: -0.0339 - rmse: 22.7044 - val_loss: 532.1771 - val_r_squared: -36.9490 - val_rmse: 21.1673\n",
      "Epoch 38/500\n",
      "1460/1460 [==============================] - 1s 956us/step - loss: 514.0690 - r_squared: -0.0238 - rmse: 22.6150 - val_loss: 522.9806 - val_r_squared: -33.9229 - val_rmse: 20.9299\n",
      "Epoch 39/500\n",
      "1460/1460 [==============================] - 1s 904us/step - loss: 514.1775 - r_squared: -0.0240 - rmse: 22.6119 - val_loss: 524.0366 - val_r_squared: -34.3165 - val_rmse: 20.9612\n",
      "Epoch 40/500\n",
      "1460/1460 [==============================] - 1s 901us/step - loss: 528.2871 - r_squared: -0.0537 - rmse: 22.8105 - val_loss: 527.7133 - val_r_squared: -35.5430 - val_rmse: 21.0545\n",
      "Epoch 41/500\n",
      "1460/1460 [==============================] - 1s 910us/step - loss: 512.4232 - r_squared: -0.0213 - rmse: 22.5801 - val_loss: 526.1696 - val_r_squared: -35.0368 - val_rmse: 21.0157\n",
      "Epoch 42/500\n",
      "1460/1460 [==============================] - 1s 945us/step - loss: 512.5947 - r_squared: -0.0219 - rmse: 22.5835 - val_loss: 525.9108 - val_r_squared: -34.9724 - val_rmse: 21.0174\n",
      "Epoch 43/500\n",
      "1460/1460 [==============================] - 1s 938us/step - loss: 512.6437 - r_squared: -0.0207 - rmse: 22.5820 - val_loss: 524.6358 - val_r_squared: -34.5358 - val_rmse: 20.9831\n",
      "Epoch 44/500\n",
      "1460/1460 [==============================] - 1s 907us/step - loss: 512.6954 - r_squared: -0.0214 - rmse: 22.5800 - val_loss: 527.1189 - val_r_squared: -35.3748 - val_rmse: 21.0494\n",
      "Epoch 45/500\n",
      "1460/1460 [==============================] - 1s 901us/step - loss: 512.7222 - r_squared: -0.0211 - rmse: 22.5848 - val_loss: 524.6025 - val_r_squared: -34.5241 - val_rmse: 20.9821\n",
      "Epoch 46/500\n",
      "1460/1460 [==============================] - 1s 910us/step - loss: 512.7307 - r_squared: -0.0212 - rmse: 22.5905 - val_loss: 521.3936 - val_r_squared: -33.3542 - val_rmse: 20.8919\n",
      "Epoch 47/500\n",
      "1460/1460 [==============================] - 1s 947us/step - loss: 512.8223 - r_squared: -0.0215 - rmse: 22.5937 - val_loss: 523.4072 - val_r_squared: -34.1014 - val_rmse: 20.9492\n",
      "Epoch 48/500\n",
      "1460/1460 [==============================] - 1s 914us/step - loss: 512.7940 - r_squared: -0.0206 - rmse: 22.5900 - val_loss: 531.7170 - val_r_squared: -36.8268 - val_rmse: 21.1670\n",
      "Epoch 49/500\n",
      "1460/1460 [==============================] - 1s 915us/step - loss: 512.7901 - r_squared: -0.0219 - rmse: 22.5803 - val_loss: 524.7263 - val_r_squared: -34.5671 - val_rmse: 20.9855\n",
      "Epoch 50/500\n",
      "1460/1460 [==============================] - 1s 899us/step - loss: 512.8634 - r_squared: -0.0218 - rmse: 22.5889 - val_loss: 527.2702 - val_r_squared: -35.4246 - val_rmse: 21.0534\n",
      "Epoch 51/500\n",
      "1460/1460 [==============================] - 1s 909us/step - loss: 512.8068 - r_squared: -0.0215 - rmse: 22.5894 - val_loss: 523.4590 - val_r_squared: -34.1200 - val_rmse: 20.9506\n",
      "Epoch 52/500\n",
      "1460/1460 [==============================] - 1s 950us/step - loss: 512.8464 - r_squared: -0.0210 - rmse: 22.5994 - val_loss: 523.1738 - val_r_squared: -34.0172 - val_rmse: 20.9427\n",
      "Epoch 53/500\n",
      "1460/1460 [==============================] - 1s 934us/step - loss: 512.8263 - r_squared: -0.0223 - rmse: 22.5887 - val_loss: 528.5606 - val_r_squared: -35.8425 - val_rmse: 21.0870\n",
      "Epoch 54/500\n",
      "1460/1460 [==============================] - 1s 979us/step - loss: 512.8055 - r_squared: -0.0208 - rmse: 22.5911 - val_loss: 527.2150 - val_r_squared: -35.4063 - val_rmse: 21.0520\n",
      "Epoch 55/500\n",
      "1460/1460 [==============================] - 1s 938us/step - loss: 512.8304 - r_squared: -0.0217 - rmse: 22.5900 - val_loss: 524.5525 - val_r_squared: -34.5067 - val_rmse: 20.9808\n",
      "Epoch 56/500\n",
      "1460/1460 [==============================] - 1s 931us/step - loss: 512.7788 - r_squared: -0.0215 - rmse: 22.5872 - val_loss: 525.8876 - val_r_squared: -34.9645 - val_rmse: 21.0168\n",
      "Epoch 57/500\n",
      "1460/1460 [==============================] - 1s 1ms/step - loss: 512.7741 - r_squared: -0.0225 - rmse: 22.5894 - val_loss: 528.2637 - val_r_squared: -35.7472 - val_rmse: 21.0793\n",
      "Epoch 58/500\n",
      "1460/1460 [==============================] - 1s 919us/step - loss: 512.7810 - r_squared: -0.0223 - rmse: 22.5845 - val_loss: 525.5139 - val_r_squared: -34.8378 - val_rmse: 21.0068\n",
      "Epoch 59/500\n",
      "1460/1460 [==============================] - 1s 930us/step - loss: 512.7542 - r_squared: -0.0217 - rmse: 22.5883 - val_loss: 529.8942 - val_r_squared: -36.2645 - val_rmse: 21.1211\n",
      "Epoch 60/500\n",
      "1460/1460 [==============================] - 1s 951us/step - loss: 512.8068 - r_squared: -0.0216 - rmse: 22.5901 - val_loss: 523.2823 - val_r_squared: -34.0563 - val_rmse: 20.9457\n",
      "Epoch 61/500\n",
      "1460/1460 [==============================] - 1s 912us/step - loss: 512.7906 - r_squared: -0.0228 - rmse: 22.5862 - val_loss: 524.5267 - val_r_squared: -34.4977 - val_rmse: 20.9801\n",
      "Epoch 62/500\n",
      "1460/1460 [==============================] - 1s 953us/step - loss: 512.8391 - r_squared: -0.0221 - rmse: 22.5886 - val_loss: 525.4210 - val_r_squared: -34.8062 - val_rmse: 21.0043\n",
      "Epoch 63/500\n",
      "1460/1460 [==============================] - 1s 943us/step - loss: 512.8672 - r_squared: -0.0214 - rmse: 22.5873 - val_loss: 530.8865 - val_r_squared: -36.5725 - val_rmse: 21.1462\n",
      "Epoch 64/500\n",
      "1460/1460 [==============================] - 2s 1ms/step - loss: 512.7220 - r_squared: -0.0214 - rmse: 22.5899 - val_loss: 533.5281 - val_r_squared: -37.3713 - val_rmse: 21.2119\n",
      "Epoch 65/500\n",
      "1460/1460 [==============================] - 1s 1ms/step - loss: 512.7692 - r_squared: -0.0219 - rmse: 22.5898 - val_loss: 520.8382 - val_r_squared: -33.1386 - val_rmse: 20.8756\n",
      "Epoch 66/500\n",
      "1460/1460 [==============================] - 1s 988us/step - loss: 512.7562 - r_squared: -0.0221 - rmse: 22.5831 - val_loss: 524.0839 - val_r_squared: -34.3425 - val_rmse: 20.9679\n",
      "Epoch 67/500\n",
      "1460/1460 [==============================] - 1s 967us/step - loss: 512.8479 - r_squared: -0.0214 - rmse: 22.5874 - val_loss: 526.6161 - val_r_squared: -35.2086 - val_rmse: 21.0362\n",
      "Epoch 68/500\n",
      "1460/1460 [==============================] - 1s 932us/step - loss: 512.7961 - r_squared: -0.0213 - rmse: 22.5908 - val_loss: 522.9450 - val_r_squared: -33.9341 - val_rmse: 20.9363\n",
      "Epoch 69/500\n",
      "1460/1460 [==============================] - 1s 1ms/step - loss: 512.8055 - r_squared: -0.0239 - rmse: 22.5862 - val_loss: 533.1218 - val_r_squared: -37.2502 - val_rmse: 21.2019\n",
      "Epoch 70/500\n",
      "1460/1460 [==============================] - 1s 923us/step - loss: 512.7889 - r_squared: -0.0209 - rmse: 22.5893 - val_loss: 528.8893 - val_r_squared: -35.9474 - val_rmse: 21.0954\n",
      "Epoch 71/500\n",
      "1460/1460 [==============================] - 1s 935us/step - loss: 512.7929 - r_squared: -0.0219 - rmse: 22.5886 - val_loss: 530.2228 - val_r_squared: -36.3670 - val_rmse: 21.1295\n",
      "Epoch 72/500\n",
      "1460/1460 [==============================] - 1s 939us/step - loss: 512.7372 - r_squared: -0.0211 - rmse: 22.5864 - val_loss: 523.1113 - val_r_squared: -33.9946 - val_rmse: 20.9409\n",
      "Epoch 73/500\n",
      "1460/1460 [==============================] - 1s 912us/step - loss: 512.7546 - r_squared: -0.0212 - rmse: 22.5873 - val_loss: 527.0557 - val_r_squared: -35.3540 - val_rmse: 21.0478\n",
      "Epoch 74/500\n",
      "1460/1460 [==============================] - 1s 916us/step - loss: 512.7908 - r_squared: -0.0237 - rmse: 22.5878 - val_loss: 529.2624 - val_r_squared: -36.0657 - val_rmse: 21.1050\n",
      "Epoch 75/500\n",
      "1460/1460 [==============================] - 1s 917us/step - loss: 512.7797 - r_squared: -0.0204 - rmse: 22.5877 - val_loss: 529.6633 - val_r_squared: -36.1921 - val_rmse: 21.1152\n",
      "Epoch 76/500\n",
      "1460/1460 [==============================] - 1s 950us/step - loss: 598.6654 - r_squared: -0.1990 - rmse: 22.8202 - val_loss: 525.8094 - val_r_squared: -34.9381 - val_rmse: 21.0147\n",
      "Epoch 77/500\n",
      "1460/1460 [==============================] - 1s 937us/step - loss: 512.6229 - r_squared: -0.0210 - rmse: 22.5839 - val_loss: 529.4169 - val_r_squared: -36.1146 - val_rmse: 21.1089\n",
      "Epoch 78/500\n",
      "1460/1460 [==============================] - 1s 915us/step - loss: 512.6873 - r_squared: -0.0221 - rmse: 22.5854 - val_loss: 530.5036 - val_r_squared: -36.4542 - val_rmse: 21.1365\n",
      "Epoch 79/500\n",
      "1460/1460 [==============================] - 1s 937us/step - loss: 512.6816 - r_squared: -0.0206 - rmse: 22.5892 - val_loss: 525.5259 - val_r_squared: -34.8420 - val_rmse: 21.0071\n",
      "Epoch 80/500\n",
      "1460/1460 [==============================] - 1s 938us/step - loss: 512.7045 - r_squared: -0.0211 - rmse: 22.5868 - val_loss: 527.9432 - val_r_squared: -35.6438 - val_rmse: 21.0710\n",
      "Epoch 81/500\n",
      "1460/1460 [==============================] - 1s 988us/step - loss: 512.7628 - r_squared: -0.0218 - rmse: 22.5857 - val_loss: 527.0272 - val_r_squared: -35.3446 - val_rmse: 21.0470\n",
      "Epoch 82/500\n",
      "1460/1460 [==============================] - 1s 971us/step - loss: 512.7334 - r_squared: -0.0210 - rmse: 22.5863 - val_loss: 526.7519 - val_r_squared: -35.2536 - val_rmse: 21.0398\n",
      "Epoch 83/500\n",
      "1460/1460 [==============================] - 1s 955us/step - loss: 512.7798 - r_squared: -0.0219 - rmse: 22.5926 - val_loss: 525.2329 - val_r_squared: -34.7419 - val_rmse: 20.9992\n",
      "Epoch 84/500\n",
      " 577/1460 [==========>...................] - ETA: 0s - loss: 512.3412 - r_squared: -0.0218 - rmse: 22.5779"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "input_shape = (x_train_tensor.shape[1],)\n",
    "\n",
    "rmse_history = RMSEHistory()\n",
    "\n",
    "baseNum = 4\n",
    "\n",
    "# Define your original model with regularization\n",
    "def build_model(input_shape, regularization_factor=0.01):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(baseNum*2, activation='relu', input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(baseNum*4, activation='elu'),\n",
    "        tf.keras.layers.Dense(baseNum*4, activation='elu'),\n",
    "        tf.keras.layers.Dense(baseNum*2, activation='linear'),\n",
    "        # tf.keras.layers.Dense(baseNum, activation='relu', kernel_regularizer=regularizers.l2(regularization_factor)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    #regular model training\n",
    "    model = build_model(input_shape)\n",
    "    model.compile(optimizer='adam', loss=mse_loss, metrics=[r_squared, rmse])\n",
    "    model.fit(x_train_tensor, y_train_tensor, epochs=500, batch_size=100, validation_split=0.2, callbacks=[early_stopping,rmse_history])\n",
    "    # if no improvement has been made in 100 generations (epochs) stop the model\n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        print(f\"Early stopping occurred at epoch {early_stopping.stopped_epoch}\")\n",
    "        print(f\"Restoring model weights from the end of the best epoch.\")\n",
    "    else:\n",
    "        print(\"Early stopping did not occur.\")\n",
    "\n",
    "    test_metrics = model.evaluate(x_test_tensor, y_test_tensor)\n",
    "    test_loss, test_accuracy, test_rmse = test_metrics[0], test_metrics[1], test_metrics[2]\n",
    "    print(f\"Test Loss (Accuracy): {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy*100}\")\n",
    "    print(f\"Test RMSE: {test_rmse}\")\n",
    "    # Plotting RMSE values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rmse_history.modelRMSE, label='Train RMSE')\n",
    "    plt.plot(rmse_history.validationRMSE, label='Validation RMSE')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('RMSE During Training')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
